{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTINEL AI - Full Dataset Training (Google Colab)\n",
    "\n",
    "This notebook trains the malware detection model on the **complete EMBER dataset** using Google Colab's resources.\n",
    "\n",
    "## Prerequisites\n",
    "1. Upload `train.parquet` to your Google Drive\n",
    "2. (Optional) Upload `test.parquet` if you have a separate test set\n",
    "\n",
    "##  Estimated Time: 30-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm pandas pyarrow scikit-learn joblib -q\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GPU Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected - Training will use CPU (slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Paths\n",
    "\n",
    "**IMPORTANT:** Update these paths to match where you uploaded your data in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update these paths!\n",
    "TRAIN_DATA_PATH = '/content/drive/MyDrive/InfoSec_Project/data/train.parquet'\n",
    "TEST_DATA_PATH = '/content/drive/MyDrive/InfoSec_Project/data/test.parquet'  # Optional\n",
    "MODEL_OUTPUT_PATH = '/content/drive/MyDrive/InfoSec_Project/models/classifier.pkl'\n",
    "METADATA_OUTPUT_PATH = '/content/drive/MyDrive/InfoSec_Project/models/model_metadata.json'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(os.path.dirname(MODEL_OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Train data: {TRAIN_DATA_PATH}\")\n",
    "print(f\"  Model output: {MODEL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "print(\"[*] Loading training data...\")\n",
    "df_train = pd.read_parquet(TRAIN_DATA_PATH, engine='pyarrow')\n",
    "\n",
    "print(f\"[*] Loaded {len(df_train):,} samples\")\n",
    "print(f\"[*] Features: {len(df_train.columns)-1}\")\n",
    "\n",
    "# Memory optimization: float64 → float32\n",
    "print(\"[*] Optimizing memory (float64 → float32)...\")\n",
    "float_cols = df_train.select_dtypes(include=['float64']).columns\n",
    "df_train[float_cols] = df_train[float_cols].astype('float32')\n",
    "\n",
    "# Show memory usage\n",
    "mem_usage = df_train.memory_usage(deep=True).sum() / 1e9\n",
    "print(f\"✓ Dataset loaded: {mem_usage:.2f} GB in memory\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Training & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "target_col = 'label'\n",
    "if 'label' not in df_train.columns:\n",
    "    candidates = [c for c in df_train.columns if 'lab' in c.lower()]\n",
    "    if candidates:\n",
    "        target_col = candidates[0]\n",
    "    else:\n",
    "        target_col = df_train.columns[-1]\n",
    "\n",
    "print(f\"[*] Target column: {target_col}\")\n",
    "\n",
    "# Split features and labels\n",
    "y_train = df_train[target_col].astype('int32')\n",
    "X_train = df_train.drop(columns=[target_col])\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n[*] Training set shape: {X_train.shape}\")\n",
    "print(f\"[*] Class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Load or split test data\n",
    "if os.path.exists(TEST_DATA_PATH):\n",
    "    print(f\"\\n[*] Loading test data from {TEST_DATA_PATH}...\")\n",
    "    df_test = pd.read_parquet(TEST_DATA_PATH, engine='pyarrow')\n",
    "    \n",
    "    # Optimize test data\n",
    "    float_cols = df_test.select_dtypes(include=['float64']).columns\n",
    "    df_test[float_cols] = df_test[float_cols].astype('float32')\n",
    "    \n",
    "    y_test = df_test[target_col].astype('int32')\n",
    "    X_test = df_test.drop(columns=[target_col])\n",
    "    del df_test\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"\\n[*] Test file not found. Splitting training data (80/20)...\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "print(f\"[*] Test set shape: {X_test.shape}\")\n",
    "print(\"\\n✓ Data prepared for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Model (This will take 30-60 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MALWARE DETECTION MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Samples: {len(X_train):,}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Configure LightGBM with GPU if available\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"[*] Training device: {device.upper()}\")\n",
    "\n",
    "if device == 'gpu':\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt',\n",
    "        device='gpu',\n",
    "        gpu_platform_id=0,\n",
    "        gpu_device_id=0,\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=256,\n",
    "        max_depth=10,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
   "else:\n",
    "    # CPU fallback\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=300,  # Slightly fewer for CPU speed\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=128,\n",
    "        max_depth=8,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train with early stopping and evaluation\n",
    "clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(period=50),\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\n[✓] Training completed: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Free training data\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ACCURACY: {acc * 100:.2f}%\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malware']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0][0]:,}\")\n",
    "print(f\"False Positives: {cm[0][1]:,}\")\n",
    "print(f\"False Negatives: {cm[1][0]:,}\")\n",
    "print(f\"True Positives: {cm[1][1]:,}\")\n",
    "\n",
    "# Additional Metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "print(f\"[*] Saving model to {MODEL_OUTPUT_PATH}...\")\n",
    "joblib.dump(clf, MODEL_OUTPUT_PATH)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'accuracy': float(acc),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'training_samples': len(y_test) * 5,  # Approximate (80/20 split)\n",
    "    'test_samples': len(y_test),\n",
    "    'features': X_test.shape[1],\n",
    "    'full_dataset': True,\n",
    "    'trained_on': 'GoogleColab',\n",
    "    'device': device,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(METADATA_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"[*] Metadata saved to {METADATA_OUTPUT_PATH}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel saved to Google Drive: {MODEL_OUTPUT_PATH}\")\n",
    "print(\"\\nNext steps:\")\n",
   "print(\"1. Download the model from Google Drive\")\n",
    "print(\"2. Place it in your local models/ directory\")\n",
    "print(\"3. Test with scanner_engine.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance.head(20)['feature'], feature_importance.head(20)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
